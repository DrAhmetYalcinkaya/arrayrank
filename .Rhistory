# 1. collect gpr data and make dataset
raw_data <- read.gpr(bgcorrect = T, fdata = "median")
#'
#' @import limma
#' @import tcltk
#' @importFrom utils choose.dir list.files
#'
#' @examples
#' \dontrun{
#' # Run the function to select a directory and read .gpr files
#' raw_data <- read.gpr()
#' }
read.gpr <- function(fdata = "mean", bgcorrect = TRUE) {
if (.Platform$OS.type == "windows") {
directory <- choose.dir()
} else {
directory <- tk_choose.dir()
}
if (is.na(directory) || directory == "") {
return(NULL)
}
files <- list.files(path = directory, pattern = "*.gpr", full.names = TRUE, include.dirs = FALSE)
if (length(files) == 0) {
message("No .gpr files found in the selected directory.")
return(NULL)
}
source_type <- if (fdata == "mean") "genepix" else "genepix.median"
raw_data <- limma::read.maimages(files, source = source_type)
raw_data$targets$FileName <- basename(raw_data$targets$FileName)
elements <- c("R", "G", "Rb", "Gb")
if (all(elements %in% names(raw_data)) && bgcorrect) {
message("Red, Green channels and background data found. Performing background correction.")
post_bg <- limma::backgroundCorrect(raw_data, method = "subtract")
} else {
message("No background correction performed.")
post_bg <- raw_data
}
return(post_bg)
}
#' \dontrun{
#' # Assuming 'raw_data' is an RGList object obtained from read.gpr()
#' Meaning run read.gpr first!
#'
#' # Extract data in long format for chambered arrays
#' long_data <- extraction(data = raw_data, array_type = "chambered", format = "long")
#'
#' # Extract data in wide format for huprot arrays
#' wide_data <- extraction(data = raw_data, array_type = "huprot", format = "wide")
#' }
extraction <- function(data, array_type, format = "wide") {
proteins <- data$genes
files <- data$targets
values <- data$R
array_count <- ncol(values)
combined_list <- vector("list", array_count)
for (i in 1:array_count) {
df <- as.data.frame(proteins)
df$array <- files[[1]][i]
df$value <- values[, i]
combined_list[[i]] <- df
}
df_out <- do.call(rbind, combined_list)
# Validate array_type parameter
if (!(array_type %in% c("chambered", "segmented", "huprot"))) {
stop("Invalid array_type. Please specify 'chambered', 'segmented', or 'huprot'.")
}
if (format == "long" & array_type != "huprot") {
long_result <- df_out %>%
dplyr::group_by(array, Block, Name) %>%
dplyr::summarise(Mvalue = mean(value, na.rm = TRUE))
output <- long_result
message("Data calculated/arranged for chambered/segmented arrays")
message("Long data created based on format input (user defined: long)")
} else if (format == "long" & array_type == "huprot") {
long_result <- df_out %>%
dplyr::group_by(array, Name, Block, Column) %>%
dplyr::summarise(Mvalue = mean(value, na.rm = TRUE)) %>%
dplyr::ungroup() %>%
dplyr::select(-Block, -Column) %>%
dplyr::group_by(array, Name) %>%
dplyr::summarise(Mvalue = if (n() >= 8) mean(Mvalue, na.rm = TRUE) else Mvalue) %>%
dplyr::mutate(Name = make.unique(as.character(Name), sep = "."))
message("Data calculated/arranged for huprot arrays")
message("Long data created based on format input (user defined: long)")
output <- long_result
} else if (format == "wide") {
if (array_type == "chambered" || array_type == "segmented") {
long_result <- df_out %>%
dplyr::group_by(array, Block, Name) %>%
dplyr::summarise(Mvalue = mean(value, na.rm = TRUE))
wide_result <- tidyr::pivot_wider(long_result, names_from = Name, values_from = Mvalue)
message("Data calculated/arranged for chambered/segmented arrays")
transposed_result <- as.data.frame(t(wide_result))
file_names <- as.character(transposed_result[1, ])
array <- paste0("Arr", sapply(basename(file_names), function(x) gsub("[^0-9]", "", x)))
new_colname <- paste0(array, "Block", transposed_result[2,])
corrected_transposed_result <- transposed_result[-c(1, 2), ]
colnames(corrected_transposed_result) <- new_colname
numeric_df <- as.data.frame(lapply(corrected_transposed_result, as.numeric))
numeric_df <- as.data.frame(apply(numeric_df, 2, function(x) round(x, 2)))
rownames(numeric_df) <- rownames(corrected_transposed_result)
} else if (array_type == "huprot") {
long_result <- df_out %>%
dplyr::group_by(array, Name, Block, Column) %>%
dplyr::summarise(Mvalue = mean(value, na.rm = TRUE)) %>%
dplyr::ungroup() %>%
dplyr::select(-Block, -Column) %>%
dplyr::group_by(array, Name) %>%
dplyr::summarise(Mvalue = if (n() >= 8) mean(Mvalue, na.rm = TRUE) else Mvalue) %>%
dplyr::mutate(Name = make.unique(as.character(Name), sep = "."))
wide_result <- tidyr::pivot_wider(long_result, names_from = Name, values_from = Mvalue)
message("Data calculated/arranged for huprot arrays. Duplicate protein names appended with .n --creating dataframe...")
transposed_result <- as.data.frame(t(wide_result))
file_names <- as.character(transposed_result[1, ])
array <- paste0("Arr", sapply(basename(file_names), function(x) gsub("[^0-9]", "", x)))
new_colname <- array
corrected_transposed_result <- as.data.frame(transposed_result[-1, ])
colnames(corrected_transposed_result) <- new_colname
row_names <- rownames(transposed_result)
row_names <- row_names[-1]
numeric_df <- as.data.frame(lapply(corrected_transposed_result, as.numeric))
numeric_df <- as.data.frame(apply(numeric_df, 2, function(x) round(x, 2)))
rownames(numeric_df) <- row_names
} else {
stop("Please define array type. Supports 'chambered', 'segmented', or 'huprot'.")
}
if (nrow(numeric_df) == 0) {
warning("The resulting data frame is empty after processing.")
}
colnames(numeric_df) <- gsub("\\.", "0", colnames(numeric_df))
output <- numeric_df
message("Wide data created based on format input (user defined: wide)")
}
return(output)
}
#' @export
#'
#' @import dplyr
#' @import tidyr
#'
#' @examples
#' \dontrun{
#' # Assuming 'raw_data' is an RGList object obtained from read.gpr()
#' discordant_duplicates <- discordant(data = raw_data, array_type = "chambered")
#' }
discordant <- function(data, array_type, fold = 1.5, abs = 500) {
df <- data$genes
merged_df <- cbind(df, data$R)
colnames(merged_df) <- basename(colnames(merged_df))
long_df <- tidyr::pivot_longer(merged_df, cols = 6:ncol(merged_df))
colnames(long_df)[6] <- "array_name"
if (array_type == "chambered" || array_type == "segmented") {
result <- long_df %>%
dplyr::group_by(array_name, Block, Name) %>%
dplyr::filter(dplyr::n() == 2) %>%
dplyr::summarize(
dup1 = dplyr::first(value),
dup2 = dplyr::last(value),
dup_mean = mean(value),
ratio = ifelse(min(value) == 0, 999, round(abs(max(value) / min(value)), 2)),
abs_diff = abs(dplyr::first(value) - dplyr::last(value)),
.groups = 'drop'
)
} else if (array_type == "huprot") {
result <- long_df %>%
dplyr::group_by(array_name, Name) %>%
dplyr::filter(dplyr::n() == 2) %>%
dplyr::summarize(
dup1 = dplyr::first(value),
dup2 = dplyr::last(value),
dup_mean = mean(value),
ratio = ifelse(min(value) == 0, 999, round(abs(max(value) / min(value)), 2)),
abs_diff = abs(dplyr::first(value) - dplyr::last(value)),
.groups = 'drop'
)
} else {
stop("Please define array_type. Accepts: huprot, chambered or segmented.")
}
flagged <- result %>%
dplyr::mutate(flag = ifelse(
(ratio > fold) &
((abs_diff > dup_mean * 0.25 & abs_diff > abs) &
(dup_mean > 250)), TRUE, FALSE))
disc <- subset(flagged, flag == TRUE)
return(disc)
}
#' \dontrun{
#' # Replace low values in the data frame with a computed value
#' corrected_data <- replace.low(data = your_data, offset = 10)
#'
#' # Replace low values with a fixed value
#' corrected_data <- replace.low(data = your_data, offset = 10, strategy = "fixed", fix = 5)
#'
#' # Replace low values with small random noise
#' corrected_data <- replace.low(data = your_data, offset = 10, strategy = "random")
#' }
replace.low <- function(data, offset = 10, strategy = "computed", fix = offset) {
data <- as.data.frame(data)
numeric_columns <- sapply(data, is.numeric)
replaced_n <- sum(data[numeric_columns] < offset, na.rm = TRUE)
if (strategy == "computed") {
data[numeric_columns][data[numeric_columns] < offset] <- abs(data[numeric_columns][data[numeric_columns] < offset] / offset + offset)
} else if (strategy == "fixed") {
data[numeric_columns][data[numeric_columns] < offset] <- fix
} else if (strategy == "random") {
data[numeric_columns][data[numeric_columns] < offset] <- runif(replaced_n, min = offset * 0.5, max = offset * 1.5)
} else {
stop("Invalid strategy. Choose 'computed', 'fixed', or 'random'.")
}
message(paste("Replaced", replaced_n, "values that are lower than the selected offset value of", offset, "with", strategy, "strategy"))
return(data)
}
#' \dontrun{
#' # Quantile normalization
#' norm_data <- multinorm(data = your_data, method = "quantile")
#'
#' # Between-array normalization
#' norm_data <- multinorm(data = your_data, method = "arrays")
#'
#' # Protein-based normalization
#' norm_data <- multinorm(data = your_data, method = "ig")
#' }
multinorm <- function(data, method = "quantile") {
data <- as.data.frame(data)
data[] <- lapply(data, as.numeric)
if (ncol(data) == 0 || nrow(data) == 0) {
stop("Data contains no valid numeric columns or rows.")
}
selected <- tolower(rownames(data)) %>%
grep("goatanti-huig", .) %>%
data[.,]
if (nrow(selected) == 0) {
selected <- tolower(rownames(data)) %>%
{data[grepl("igg", .) & grepl("100ng/ul", .), ]}
}
ortalamalar <- apply(selected, 2, mean)
beta <- median(unlist(selected))
logistic_trans <- Vectorize(function(x, b = beta, alt = 0.8, ust = 1.8) {
if (x < b/4) {
a = 0.009
return(1 / (1 + exp(-a * (x - b))))
} else {
a = 0.001
return(alt + (ust - alt) / (1 + exp(-a * (b - x))))
}
})
log_trans <- logistic_trans(ortalamalar)
binary <- ifelse(log_trans > 0.01, TRUE, FALSE)
if (sum(!binary) > 0) {
post_data <- data[, binary, drop = FALSE]
backup <- data[, !binary, drop = FALSE]
message("Extremely low Ig responses detected in: ", paste(colnames(data)[!binary], collapse = ", "),
"\nHighly suggested to exclude these indices from the data. Normalization will continue without altering these columns.")
} else {
post_data <- data
backup <- NULL
message("No columns identified as negative controls. Proceeding with normalization.")
}
if (method == "quantile") {
normed <- as.data.frame(round(limma::normalizeQuantiles(post_data), 1))
data[, binary] <- normed
output <- round(data, 1)
message("Normalized each column (observations) with limma-Quantile.")
} else if (method == "arrays") {
normed <- as.data.frame(round(limma::normalizeBetweenArrays(post_data), 1))
data[, binary] <- normed
output <- round(data, 1)
message("Normalized between arrays (Blocks or Huprot) with limma-BetweenArrays.")
} else if (method == "ig") {
df <- data
for (i in 1:ncol(df)) {
k <- ifelse(log_trans < 0.1, 1, log_trans)
df[, i] <- data[, i] * k[i]
output <- round(df, 1)
}
message("Normalized for Ig values.")
} else {
message("Please define normalization variables correctly.")
}
return(output)
}
#' @return A data frame with an additional column for protein names, ready for export to Excel or other formats.
#' @export
#'
#' @import dplyr
#'
#' @examples
#' \dontrun{
#' # Example usage of store.df function
#' data_to_store <- store.df(data = your_data)
#' }
store.df <- function(data) {
protein_names <- rownames(data)
output <- cbind(protein_names, data)
message("Storable dataframe created for export into Excel.")
return(output)
}
#'
#' @import dplyr
#'
#' @examples
#' \dontrun{
#' # Example usage of detect.hits function
#' ranked_proteins <- detect.hits(data = my_data,
#'                                controls = "Control",
#'                                examine = c("Treatment1", "Treatment2"))
#' }
detect.hits <- function(data, group_vector = NULL, controls, examine = NULL,
sdmean = 0.7, min_mean = 250,
fold_threshold = 10, absolute_threshold = 7000,
fold_shared = 1, abs_shared = 1) {
# Convert to a data frame if it's not already (handling tibbles)
if ("tbl_df" %in% class(data)) {
data <- as.data.frame(data)
}
# Attempt to convert the first row (excluding the first column) to numeric
first_row_numeric <- suppressWarnings(as.numeric(as.character(data[1, -1])))
is_first_row_numeric <- !any(is.na(first_row_numeric))
# If group_vector is provided, use it; otherwise, decide based on the first row content
if (!is.null(group_vector)) {
if (length(group_vector) != ncol(data) - 1) {
stop("The length of 'group_vector' must match the number of samples in the data (columns except the protein name column).")
}
cat("A group_vector has been provided.\nAnalysis will assume the first row contains array data.\n\n")
groups <- as.character(group_vector)
proteins <- as.character(data[, 1])
data <- data[, -1]
} else if (is_first_row_numeric) {
cat("The first row is numeric (array data?) and no group_vector has been provided.\nINFO: This function can accept groups as a character vector (group_vector = '')\nor as character entries in the first row of the dataset.\nPlease provide group info as such.\n")
stop()
} else {
message("The first row appears to contain group data, analyzing based on these groups")
groups <- as.character(data[1, -1])
proteins <- as.character(data[-c(1), 1])
data <- data[-c(1), -1]
}
# Convert column data to numeric and define proteins as row names
data[] <- lapply(data, as.numeric)
rownames(data) <- proteins
print(table(groups))
# Identify the control group indices
control_indices <- which(groups == controls)
if (length(control_indices) == 0) {
stop("The specified control group was not found in the group annotations.")
}
# If no specific non-control groups are provided, use all except the control group
if (is.null(examine)) {
non_control_indices <- which(groups != controls)
} else {
non_control_indices <- which(groups %in% examine)
}
if (length(non_control_indices) == 0) {
stop("No valid non-control groups were found based on your criteria.")
}
# Check and print out groups being analyzed
message(paste("Selected control group:", controls))
message(paste("Selected analysis group:", paste(unique(groups[non_control_indices]), collapse=", ")))
# Check if the standard deviation-to-mean ratio is greater than sdmean value
variation <- function(row, ratio) {
sd_to_mean_ratio <- sd(row) / mean(row)
return(sd_to_mean_ratio > ratio)
}
# Filter rows based on variance in the data
filter_variance <- apply(as.matrix(data[, non_control_indices]), 1, variation, sdmean)
edf2 <- data[filter_variance, ]
proteins <- proteins[filter_variance]  # adjust the 'proteins' list with the same filter
message(paste("Filtered data based on variance:", nrow(edf2)))
# Filter rows based on the mean value of each row in the non-control data
filter_minmean <- apply(edf2[, non_control_indices], 1, function(row) mean(row) > min_mean)
edf3 <- edf2[filter_minmean, ]
proteins <- proteins[filter_minmean]
message(paste("Filtered data based on mean value:", nrow(edf3)))
# Calculate the mean value for controls while excluding the highest value
edf3$ControlMean <- apply(edf3[, control_indices], 1, function(row) mean(row[row != max(row)]))
# Exclude proteins where controls have higher values compared to the max value in non-control samples
filter_greatercontrols <- edf3$ControlMean < apply(edf3[, non_control_indices], 1, max)
edf4 <- edf3[filter_greatercontrols, ]
proteins <- proteins[filter_greatercontrols]
message(paste("Filtered data based on greater-than-controls check:", nrow(edf4)))
# Filter based on fold change
filter_folddif <- rowSums(edf4[, non_control_indices] > fold_threshold * edf4$ControlMean) > (fold_shared - 1)
edf5 <- edf4[filter_folddif, ]
proteins <- proteins[filter_folddif]
message(paste("Filtered data based on fold difference:", nrow(edf5)))
# Filter based on absolute difference
filter_absdif <- rowSums(edf5[, non_control_indices] > absolute_threshold) > (abs_shared - 1)
edf6 <- edf5[filter_absdif, ]
proteins <- proteins[filter_absdif]
message(paste("Filtered data based on absolute difference:", nrow(edf6)))
# Extract absolute values of non-control samples
abs_values_non_controls <- edf6[, non_control_indices]
# Calculate fold change for each protein
fold_changes <- abs_values_non_controls / edf6$ControlMean
# Number of hits
number_of_hits <- rowSums((fold_changes > fold_threshold) & (abs_values_non_controls > absolute_threshold))
# Base ranking score for each non-control
fold_changes_with_abslog <- (fold_changes * log10(abs_values_non_controls))
# Order for raw foldchange and basescore data -- separately
fold_change_values <- t(apply(fold_changes, 1, function(row) row[order(-row)]))
score_foldchange_and_log10 <- t(apply(fold_changes_with_abslog, 1, function(row) row[order(-row)]))
# Sum the scores for each non-control to create final ranking score
rankingscore <- (rowSums(score_foldchange_and_log10) * number_of_hits * log10(edf6$ControlMean))
message("Rank scores calculated")
# Add ranking score, highest fold change, number of hits, and BDmean to the dataframe
abs_values_non_controls$highest_fold_change <- fold_change_values[, 1]
abs_values_non_controls$potential_hits <- number_of_hits
abs_values_non_controls$control_mean <- edf6$ControlMean
abs_values_non_controls$rankingscore <- rankingscore
message("Supportive info added.")
# Add a rank column to the dataframe (convenience)
abs_values_non_controls$rank <- rank(-abs_values_non_controls$rankingscore)
# Match the length of 'Proteins' to the filtered data
if (length(proteins) != nrow(abs_values_non_controls)) {
stop("Mismatch in the number of rows after final filtering. Perhaps check the data for format or structure?")
}
# Combine results with protein and group information
results <- data.frame(proteins, abs_values_non_controls, check.names = FALSE)
ranked_prots <- results[order(results$rank), ]
return(ranked_prots)
}
# 1. collect gpr data and make dataset
raw_data <- read.gpr(bgcorrect = T, fdata = "median")
wide_df <- extraction(raw_data, array_type = "chambered", format = "wide")
wide_df <- extraction(raw_data, array_type = "chambered", format = "wide")
library(dplyr)
wide_df <- extraction(raw_data, array_type = "chambered", format = "wide")
# 1b. identify discordant duplicates in data (to cross-check with ultimate output)
discordants <- discordant(raw_data, array_type = "chambered", fold = 2, abs = 1000)
# 2. data correction
corr_df <- replace.low(wide_df, offset = 10)
qnorm_df <- multinorm(corr_df, method= "ig")
# 3. storage and backup
qstore_df <- store.df(qnorm_df)
# 4. read data from backup file
q_data <- readxl::read_xlsx(choose.files())
# 5. read group info from excel (if you want to provide a vector defining the sample groups)
groups <- readxl::read_xlsx(choose.files())
groupsv <- t(groups[,2])
# 6. detect proteins with hits and rank them (in this example: the groups, BD and SSc, have been provided as a vector)
q_result1 <- detect.hits(q_data, controls = "BD", group_vector = groupsv, examine = "Pan", sdmean = 0.5,
fold_threshold = 5, absolute_threshold = 5000)
# 1. collect gpr data and make dataset
raw_data <- read.gpr(bgcorrect = T, fdata = "median")
wide_df <- extraction(raw_data, array_type = "huprot", format = "wide")
# 2. data correction
corr_df <- replace.low(wide_df, offset = 10)
qnorm_df <- multinorm(corr_df, method= "ig")
# 3. storage and backup
qstore_df <- store.df(qnorm_df)
View(qstore_df)
View(qstore_df)
library(arrayrank)
# 1. collect gpr data and make dataset
raw_data <- read.gpr(bgcorrect = T, fdata = "median")
wide_df <- extraction(raw_data, array_type = "chambered", format = "wide")
View(wide_df)
wide_df <- extraction(raw_data, array_type = "huprot", format = "wide")
View(wide_df)
# 1. collect gpr data and make dataset
raw_data <- read.gpr(bgcorrect = T, fdata = "median")
wide_df <- extraction(raw_data, array_type = "huprot", format = "wide")
library(devtools)
remove.packages("arrayrank")
detach("package:arrayrank", unload = TRUE)
devtools::install_github("DrAhmetYalcinkaya/arrayrank")
devtools::install_github("DrAhmetYalcinkaya/arrayrank")
wide_df2 <- extraction(raw_data, array_type = "huprot", format = "wide")
library(arrayrank)
wide_df2 <- extraction(raw_data, array_type = "huprot", format = "wide")
wide_df2 <- extraction(raw_data, array_type = "chambered", format = "wide")
View(wide_df2)
devtools::install_github("DrAhmetYalcinkaya/arrayrank")
# 1. collect gpr data and make dataset
raw_data <- read.gpr(bgcorrect = T, fdata = "median")
wide_df <- extraction(raw_data, array_type = "chambered", format = "wide")
View(wide_df)
View(wide_df2)
View(wide_df)
library(arrayrank)
# 1. collect gpr data and make dataset
raw_data <- read.gpr(bgcorrect = T, fdata = "median")
wide_df <- extraction(raw_data, array_type = "chambered", format = "wide")
# 1b. identify discordant duplicates in data (to cross-check with ultimate output)
discordants <- discordant(raw_data, array_type = "chambered", fold = 2, abs = 1000)
# 2. data correction
corr_df <- replace.low(wide_df, offset = 10)
qnorm_df <- multinorm(corr_df, method= "ig")
View(qnorm_df)
max(qnorm_df)
# 3. storage and backup
qstore_df <- store.df(qnorm_df)
writexl::write_xlsx(qstore_df, "Sigmoid_normed_casanova.xlsx")
# 4. read data from backup file
q_data <- readxl::read_xlsx(choose.files())
# 5. read group info from excel (if you want to provide a vector defining the sample groups)
groups <- readxl::read_xlsx(choose.files())
View(groups)
groupsv <- t(groups[,2]) ### note: read/create/edit the character vector based on your own requirements!!!
# 6. detect proteins with hits and rank them (in this example: the groups, BD and SSc, have been provided as a vector)
q_result1 <- detect.hits(q_data, controls = "BD", group_vector = groupsv, examine = "Pan", sdmean = 0.5,
fold_threshold = 5, absolute_threshold = 5000)
# 6. detect proteins with hits and rank them (in this example: the groups, BD and SSc, have been provided as a vector)
q_result1 <- detect.hits(q_data, controls = "BD", group_vector = groupsv, sdmean = 0.5,
fold_threshold = 5, absolute_threshold = 5000)
# 6. detect proteins with hits and rank them (in this example: the groups, BD and SSc, have been provided as a vector)
q_result <- detect.hits(q_data, controls = "BD", group_vector = groupsv, sdmean = 0.5,
fold_threshold = 5, absolute_threshold = 5000)
writexl::write_xlsx(q_result, "Hits_sigmoidnormed_casanova.xlsx")
# 6. detect proteins with hits and rank them (in this example: the groups, BD and SSc, have been provided as a vector)
q_result <- detect.hits(q_data, controls = "BD", group_vector = groupsv, sdmean = 0.5,
fold_threshold = 10, absolute_threshold = 5000)
View(q_result)
