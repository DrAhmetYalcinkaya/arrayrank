qstore_df <- store.df(qnorm_df)
View(qstore_df)
View(qstore_df)
library(arrayrank)
# 1. collect gpr data and make dataset
raw_data <- read.gpr(bgcorrect = T, fdata = "median")
wide_df <- extraction(raw_data, array_type = "chambered", format = "wide")
View(wide_df)
wide_df <- extraction(raw_data, array_type = "huprot", format = "wide")
View(wide_df)
# 1. collect gpr data and make dataset
raw_data <- read.gpr(bgcorrect = T, fdata = "median")
wide_df <- extraction(raw_data, array_type = "huprot", format = "wide")
library(devtools)
remove.packages("arrayrank")
detach("package:arrayrank", unload = TRUE)
devtools::install_github("DrAhmetYalcinkaya/arrayrank")
devtools::install_github("DrAhmetYalcinkaya/arrayrank")
wide_df2 <- extraction(raw_data, array_type = "huprot", format = "wide")
library(arrayrank)
wide_df2 <- extraction(raw_data, array_type = "huprot", format = "wide")
wide_df2 <- extraction(raw_data, array_type = "chambered", format = "wide")
View(wide_df2)
devtools::install_github("DrAhmetYalcinkaya/arrayrank")
# 1. collect gpr data and make dataset
raw_data <- read.gpr(bgcorrect = T, fdata = "median")
wide_df <- extraction(raw_data, array_type = "chambered", format = "wide")
View(wide_df)
View(wide_df2)
View(wide_df)
library(arrayrank)
# 1. collect gpr data and make dataset
raw_data <- read.gpr(bgcorrect = T, fdata = "median")
wide_df <- extraction(raw_data, array_type = "chambered", format = "wide")
# 1b. identify discordant duplicates in data (to cross-check with ultimate output)
discordants <- discordant(raw_data, array_type = "chambered", fold = 2, abs = 1000)
# 2. data correction
corr_df <- replace.low(wide_df, offset = 10)
qnorm_df <- multinorm(corr_df, method= "ig")
View(qnorm_df)
max(qnorm_df)
# 3. storage and backup
qstore_df <- store.df(qnorm_df)
writexl::write_xlsx(qstore_df, "Sigmoid_normed_casanova.xlsx")
# 4. read data from backup file
q_data <- readxl::read_xlsx(choose.files())
# 5. read group info from excel (if you want to provide a vector defining the sample groups)
groups <- readxl::read_xlsx(choose.files())
View(groups)
groupsv <- t(groups[,2]) ### note: read/create/edit the character vector based on your own requirements!!!
# 6. detect proteins with hits and rank them (in this example: the groups, BD and SSc, have been provided as a vector)
q_result1 <- detect.hits(q_data, controls = "BD", group_vector = groupsv, examine = "Pan", sdmean = 0.5,
fold_threshold = 5, absolute_threshold = 5000)
# 6. detect proteins with hits and rank them (in this example: the groups, BD and SSc, have been provided as a vector)
q_result1 <- detect.hits(q_data, controls = "BD", group_vector = groupsv, sdmean = 0.5,
fold_threshold = 5, absolute_threshold = 5000)
# 6. detect proteins with hits and rank them (in this example: the groups, BD and SSc, have been provided as a vector)
q_result <- detect.hits(q_data, controls = "BD", group_vector = groupsv, sdmean = 0.5,
fold_threshold = 5, absolute_threshold = 5000)
writexl::write_xlsx(q_result, "Hits_sigmoidnormed_casanova.xlsx")
# 6. detect proteins with hits and rank them (in this example: the groups, BD and SSc, have been provided as a vector)
q_result <- detect.hits(q_data, controls = "BD", group_vector = groupsv, sdmean = 0.5,
fold_threshold = 10, absolute_threshold = 5000)
View(q_result)
library(arrayrank)
# 1. collect gpr data and make dataset
raw_data <- read.gpr(bgcorrect = T, fdata = "median")
wide_df <- extraction(raw_data, array_type = "huprot", format = "wide")
View(wide_df)
# 2. data correction
corr_df <- replace.low(wide_df, offset = 10)
View(corr_df)
View(wide_df)
View(corr_df)
min(wide_df)
which(min(wide_df))
which(wide_df == min(wide_df))
which(wide_df == min(wide_df), indices = T)
which(wide_df == min(wide_df), index = T)
?which()
which(wide_df == min(wide_df), ind = T)
which(wide_df == min(wide_df), arr.ind = T)
# 4. read data from backup file
q_data <- readxl::read_xlsx(choose.files())
# 4. read data from backup file
q_data <- readxl::read_xlsx(choose.files())
View(q_data)
# 4. read data from backup file
q_data <- readxl::read_xlsx(choose.files())
View(q_data)
# 4. read data from backup file
q_data <- readxl::read_xlsx(choose.files())
View(q_data)
# 4. read data from backup file
q_data <- readxl::read_xlsx(choose.files())
View(q_data)
# 6. detect proteins with hits and rank them (in this example: the groups have been provided as a vector)
q_result <- detect.hits(q_data, controls = "BD",
examine = "SSc", sdmean = 0.5, fold_threshold = 10, absolute_threshold = 7000)
library(arrayrank)
# 6. detect proteins with hits and rank them (in this example: the groups have been provided as a vector)
q_result <- detect.hits(q_data, controls = "BD",
examine = "SSc", sdmean = 0.5, fold_threshold = 10, absolute_threshold = 7000)
View(q_result)
# 6. detect proteins with hits and rank them (in this example: the groups have been provided as a vector)
q_result_2k <- detect.hits(q_data, controls = "BD",
examine = "SSc", sdmean = 0.5, fold_threshold = 10, absolute_threshold = 2000)
View(q_result_2k)
View(q_result)
View(q_result_2k)
View(q_result)
View(q_result)
View(q_result)
View(q_result_2k)
# 6. detect proteins with hits and rank them (in this example: the groups have been provided as a vector)
q_result_2k2 <- detect.hits(q_data, controls = "BD",
examine = "SSc", sdmean = 0.5, fold_threshold = 4, absolute_threshold = 2000)
View(q_result_2k2)
View(q_result_2k2)
# 6. detect proteins with hits and rank them (in this example: the groups have been provided as a vector)
q_result_2k2 <- detect.hits(q_data, controls = "BD",
examine = "SSc", sdmean = 0.5, fold_threshold = 2, absolute_threshold = 1000)
View(q_result_2k2)
library(arrayrank)
# 1. collect gpr data and make dataset
raw_data <- read.gpr(bgcorrect = T, fdata = "median")
wide_df <- extraction(raw_data, array_type = "chambered", format = "wide")
View(wide_df)
# 2. data correction
corr_df <- replace.low(wide_df, offset = 10)
qnorm_df <- multinorm(corr_df, method= "ig")
data <- corr_df
if (ncol(data) == 0 || nrow(data) == 0) {
stop("Data contains no valid numeric columns or rows.")
}
selected <- tolower(rownames(data)) %>%
grep("goatanti-huig", .) %>%
data[.,]
#' \dontrun{
#' # Quantile normalization
#' norm_data <- multinorm(data = your_data, method = "quantile")
#'
#' # Between-array normalization
#' norm_data <- multinorm(data = your_data, method = "arrays")
#'
#' # Protein-based normalization
#' norm_data <- multinorm(data = your_data, method = "ig")
#' }
library(dplyr)
selected <- tolower(rownames(data)) %>%
grep("goatanti-huig", .) %>%
data[.,]
View(selected)
View(corr_df)
if (nrow(selected) == 0) {
selected <- tolower(rownames(data)) %>%
{data[grepl("igg", .) & grepl("100ng/ul", .), ]}
}
selected <- tolower(rownames(data)) %>%
grep("hu", .) %>%
data[.,]
View(selected)
selected <- tolower(rownames(data)) %>%
grep("hu IgG", .) %>%
data[.,]
View(selected)
selected <- tolower(rownames(data)) %>%
grep("hu igg", .) %>%
data[.,]
selected <- tolower(rownames(data)) %>%
grep("hu igg", .) %>%
data[.,]
selected <- tolower(rownames(data)) %>%
grep("hu", .) %>%
data[.,]
View(selected)
selected <- tolower(rownames(data)) %>%
grep("hu igg", .) %>%
data[.,]
View(selected)
selected <- tolower(rownames(data)) %>%
grep("igg", .) %>%
data[.,]
View(selected)
selected <- tolower(rownames(data)) %>%
grep("anti-hu igg", .) %>%
data[.,]
if (nrow(selected) == 0) {
selected <- tolower(rownames(data)) %>%
{data[grepl("igg", .) & grepl("100ng/ul", .), ]}
}
#' \dontrun{
#' # Quantile normalization
#' norm_data <- multinorm(data = your_data, method = "quantile")
#'
#' # Between-array normalization
#' norm_data <- multinorm(data = your_data, method = "arrays")
#'
#' # Protein-based normalization
#' norm_data <- multinorm(data = your_data, method = "ig")
#' }
multinorm <- function(data, method = "quantile") {
data <- as.data.frame(data)
data[] <- lapply(data, as.numeric)
if (ncol(data) == 0 || nrow(data) == 0) {
stop("Data contains no valid numeric columns or rows.")
}
selected <- tolower(rownames(data)) %>%
grep("anti-hu igg", .) %>%
data[.,]
if (nrow(selected) == 0) {
selected <- tolower(rownames(data)) %>%
{data[grepl("igg", .) & grepl("100ng/ul", .), ]}
}
ortalamalar <- apply(selected, 2, mean)
beta <- median(unlist(selected))
logistic_trans <- Vectorize(function(x, b = beta, alt = 0.8, ust = 1.8) {
if (x < b/4) {
a = 0.009
return(1 / (1 + exp(-a * (x - b))))
} else {
a = 0.001
return(alt + (ust - alt) / (1 + exp(-a * (b - x))))
}
})
log_trans <- logistic_trans(ortalamalar)
binary <- ifelse(log_trans > 0.01, TRUE, FALSE)
if (sum(!binary) > 0) {
post_data <- data[, binary, drop = FALSE]
backup <- data[, !binary, drop = FALSE]
message("Extremely low Ig responses detected in: ", paste(colnames(data)[!binary], collapse = ", "),
"\nHighly suggested to exclude these indices from the data. Normalization will continue without altering these columns.")
} else {
post_data <- data
backup <- NULL
message("No columns identified as negative controls. Proceeding with normalization.")
}
if (method == "quantile") {
normed <- as.data.frame(round(limma::normalizeQuantiles(post_data), 1))
data[, binary] <- normed
output <- round(data, 1)
message("Normalized each column (observations) with limma-Quantile.")
} else if (method == "arrays") {
normed <- as.data.frame(round(limma::normalizeBetweenArrays(post_data), 1))
data[, binary] <- normed
output <- round(data, 1)
message("Normalized between arrays (Blocks or Huprot) with limma-BetweenArrays.")
} else if (method == "ig") {
df <- data
for (i in 1:ncol(df)) {
k <- ifelse(log_trans < 0.1, 1, log_trans)
df[, i] <- data[, i] * k[i]
output <- round(df, 1)
}
message("Normalized for Ig values.")
} else {
message("Please define normalization variables correctly.")
}
return(output)
}
qnorm_df <- multinorm(corr_df, method= "ig")
View(qnorm_df)
max(qnorm_df)
# 3. storage and backup
qstore_df <- store.df(qnorm_df)
writexl::write_xlsx(qstore_df, "new_array_output.xlsx")
selected <- tolower(rownames(data)) %>%
grep("anti", .) %>%
data[.,]
View(selected)
selected <- tolower(rownames(data)) %>%
grep("anti-hu igg", .) %>%
data[.,]
library(devtools)
remove.packages("arrayrank")
detach("package:arrayrank", unload = TRUE)
devtools::install_github("DrAhmetYalcinkaya/arrayrank")
install.packages("limma")
devtools::install_github("DrAhmetYalcinkaya/arrayrank")
if (!require("BiocManager", quietly = TRUE))
install.packages("BiocManager")
BiocManager::install("limma")
devtools::install_github("DrAhmetYalcinkaya/arrayrank")
library(arrayrank)
# 1. collect gpr data and make dataset
raw_data <- read.gpr(bgcorrect = T, fdata = "median")
# Load necessary library
library(data.table)
# Specify the directory containing the .gpr files
input_directory <- choose.dir()
# Specify the directory to save the modified files
output_directory <- choose.dir()
# Create the output directory if it doesn't exist
if (!dir.exists(output_directory)) {
dir.create(output_directory)
}
# Get a list of all .gpr files in the input directory
gpr_files <- list.files(input_directory, pattern = "\\.gpr$", full.names = T)
# Function to remove content after '=' for specific keys
remove_content_after_equal <- function(file) {
# Read all lines from the file, trying to fix any encoding issues
lines <- readLines(file, warn = FALSE, encoding = "latin1")
# Convert the lines to UTF-8 encoding, skipping invalid characters
lines <- iconv(lines, from = "latin1", to = "UTF-8", sub = "")
# Replace content after "=" for the specified fields
lines <- gsub("ImageFiles=.*", "ImageFiles=", lines)
lines <- gsub("JpegImage=.*", "JpegImage=", lines)
# Construct the new file path in the output directory
output_file <- file.path(output_directory, basename(file))
# Write the modified lines to the new file
writeLines(lines, output_file)
}
# Apply the function to all .gpr files
lapply(gpr_files, remove_content_after_equal)
print("Content after '=' removed in specified cells and saved to new files.")
# 1. collect gpr data and make dataset
raw_data <- read.gpr(bgcorrect = T, fdata = "median")
wide_df <- extraction(raw_data, array_type = "huprot", format = "wide")
# 1b. identify discordant duplicates in data (to cross-check with ultimate output)
discordants <- discordant(raw_data, array_type = "huprot", fold = 4, abs = 2000)
View(wide_df)
View(discordants)
# 2. data correction
corr_df <- replace.low(wide_df, offset = 10)
qnorm_df <- multinorm(corr_df, method= "ig")
# 3. storage and backup
qstore_df <- store.df(qnorm_df)
writexl::write_xlsx(qstore_df, "new_huprots_list.xlsx")
library(arrayrank)
# 1. collect gpr data and make dataset
raw_data <- read.gpr(bgcorrect = T, fdata = "median")
wide_df <- extraction(raw_data, array_type = "huprot", format = "wide")
min(wide_df)
# 2. data correction
corr_df <- replace.low(wide_df, offset = 10)
# 3. storage and backup
qstore_df <- store.df(qnorm_df)
writexl::write_xlsx(qstore_df, "SASH_and_RELB_analysis_with_BDs.xlsx")
# 3. storage and backup
qstore_df <- store.df(qnorm_df)
qnorm_df <- multinorm(corr_df, method= "ig")
# 3. storage and backup
qstore_df <- store.df(qnorm_df)
writexl::write_xlsx(qstore_df, "SASH_and_RELB_analysis_with_BDs.xlsx")
# 5. read group info from excel (if you want to provide a vector defining the sample groups)
groups <- readxl::read_xlsx(choose.files())
groupsv <- t(groups[,2]) ### note: read/create/edit the character vector based on your own requirements!!!
View(qstore_df)
View(qnorm_df)
# 5. read group info from excel (if you want to provide a vector defining the sample groups)
groups <- readxl::read_xlsx(choose.files())
groupsv <- t(groups[,2]) ### note: read/create/edit the character vector based on your own requirements!!!
# 6. detect proteins with hits and rank them (in this example: the groups have been provided as a vector)
q_result <- detect.hits(q_data, controls = "BD", group_vector = groupsv,
examine = "SASH3", sdmean = 0.5, fold_threshold = 5, absolute_threshold = 5000)
# 4. read data from backup file
q_data <- readxl::read_xlsx(choose.files())
View(q_data)
# 6. detect proteins with hits and rank them (in this example: the groups have been provided as a vector)
q_result <- detect.hits(q_data, controls = "BD", group_vector = groupsv,
examine = "SASH3", sdmean = 0.5, fold_threshold = 5, absolute_threshold = 5000)
View(q_result)
# 6. detect proteins with hits and rank them (in this example: the groups have been provided as a vector)
SASH3_results <- detect.hits(q_data, controls = "BD", group_vector = groupsv,
examine = "SASH3", sdmean = 0.5, fold_threshold = 10, absolute_threshold = 5000)
View(SASH3_results)
qnorm_df <- multinorm(corr_df, method= "quantile")
# 3. storage and backup
qstore_df <- store.df(qnorm_df)
writexl::write_xlsx(qstore_df, "SASH_RELB_IPEX_analysis_with_BDs_Qnorm.xlsx")
# 4. read data from backup file
q_data <- readxl::read_xlsx(choose.files())
# 6. detect proteins with hits and rank them (in this example: the groups have been provided as a vector)
SASH3_results_qnorm <- detect.hits(q_data, controls = "BD", group_vector = groupsv,
examine = "SASH3", sdmean = 0.5, fold_threshold = 10, absolute_threshold = 5000)
View(SASH3_results_qnorm)
View(wide_df)
qnorm_df <- multinorm(corr_df, method= "quantile")
# 3. storage and backup
qstore_df <- store.df(qnorm_df)
writexl::write_xlsx(qstore_df, "SASH_RELB_IPEX_analysis_with_BDs_Qnorm.xlsx")
# 4. read data from backup file
q_data <- readxl::read_xlsx(choose.files())
# 6. detect proteins with hits and rank them (in this example: the groups have been provided as a vector)
SASH3_results_qnorm <- detect.hits(q_data, controls = "BD", group_vector = groupsv,
examine = "SASH3", sdmean = 0.5, fold_threshold = 10, absolute_threshold = 5000)
View(q_data)
View(SASH3_results_qnorm)
# 6. detect proteins with hits and rank them (in this example: the groups have been provided as a vector)
SASH3_results_qnorm <- detect.hits(q_data, controls = "BD", group_vector = groupsv,
examine = "SASH3", sdmean = 0.5, fold_threshold = 10, absolute_threshold = 5000)
# 4. read data from backup file
q_data <- readxl::read_xlsx(choose.files())
# 6. detect proteins with hits and rank them (in this example: the groups have been provided as a vector)
SASH3_results_ignorm <- detect.hits(q_data, controls = "BD", group_vector = groupsv,
examine = "SASH3", sdmean = 0.5, fold_threshold = 10, absolute_threshold = 5000)
View(SASH3_results_ignorm)
# 2. data correction
corr_df <- replace.low(wide_df, offset = 10)
# 1. collect gpr data and make dataset
raw_data <- read.gpr(bgcorrect = T, fdata = "median")
wide_df <- extraction(raw_data, array_type = "huprot", format = "wide")
# 2. data correction
corr_df <- replace.low(wide_df, offset = 10)
qnorm_df <- multinorm(corr_df, method= "quantile")
ignorm_df <- multinorm(corr_df, method= "ig")
# 3. storage and backup
qstore_df <- store.df(qnorm_df)
writexl::write_xlsx(qstore_df, "SASH_RELB_IPEX_analysis_with_BDs_qnorm.xlsx")
igstore_df <- store.df(ignorm_df)
View(igstore_df)
View(ignorm_df)
writexl::write_xlsx(qstore_df, "SASH_RELB_IPEX_analysis_with_BDs_ignorm.xlsx")
writexl::write_xlsx(igstore_df, "SASH_RELB_IPEX_analysis_with_BDs_ignorm.xlsx")
View(igstore_df)
View(qstore_df)
View(igstore_df)
# 3. storage and backup
qstore_df <- store.df(qnorm_df)
writexl::write_xlsx(qstore_df, "SASH_RELB_IPEX_analysis_with_BDs_qnorm.xlsx")
igstore_df <- store.df(ignorm_df)
writexl::write_xlsx(igstore_df, "SASH_RELB_IPEX_analysis_with_BDs_ignorm.xlsx")
#########################
# 4. read data from backup file
q_data <- readxl::read_xlsx(choose.files())
ig_data <- readxl::read_xlsx(choose.files())
# 5. read group info from excel (if you want to provide a vector defining the sample groups)
groups <- readxl::read_xlsx(choose.files())
groupsv <- t(groups[,2]) ### note: read/create/edit the character vector based on your own requirements!!!
############### Results
# SASH3 qnorm
SASH3_results_qnorm <- detect.hits(q_data, controls = "BD", group_vector = groupsv,
examine = "SASH3", sdmean = 0.5, fold_threshold = 5, absolute_threshold = 5000)
# SASH3 ignorm
SASH3_results_ignorm <- detect.hits(ig_data, controls = "BD", group_vector = groupsv,
examine = "SASH3", sdmean = 0.5, fold_threshold = 5, absolute_threshold = 5000)
# RELB qnorm
SASH3_results_qnorm <- detect.hits(q_data, controls = "BD", group_vector = groupsv,
examine = "RELB", sdmean = 0.5, fold_threshold = 5, absolute_threshold = 5000)
# RELB ignorm
SASH3_results_ignorm <- detect.hits(ig_data, controls = "BD", group_vector = groupsv,
examine = "RELB", sdmean = 0.5, fold_threshold = 5, absolute_threshold = 5000)
############### Results
# SASH3 qnorm
SASH3_results_qnorm <- detect.hits(q_data, controls = "BD", group_vector = groupsv,
examine = "SASH3", sdmean = 0.5, fold_threshold = 5, absolute_threshold = 5000)
# SASH3 ignorm
SASH3_results_ignorm <- detect.hits(ig_data, controls = "BD", group_vector = groupsv,
examine = "SASH3", sdmean = 0.5, fold_threshold = 5, absolute_threshold = 5000)
# RELB qnorm
RELB_results_qnorm <- detect.hits(q_data, controls = "BD", group_vector = groupsv,
examine = "RELB", sdmean = 0.5, fold_threshold = 5, absolute_threshold = 5000)
# RELB ignorm
RELB_results_ignorm <- detect.hits(ig_data, controls = "BD", group_vector = groupsv,
examine = "RELB", sdmean = 0.5, fold_threshold = 5, absolute_threshold = 5000)
#IPEX qnorm
IPEX_results_qnorm <- detect.hits(q_data, controls = "BD", group_vector = groupsv,
examine = "IPEX", sdmean = 0.5, fold_threshold = 5, absolute_threshold = 5000)
#IPEX ignorm
IPEX_results_ignorm <- detect.hits(ig_data, controls = "BD", group_vector = groupsv,
examine = "IPEX", sdmean = 0.5, fold_threshold = 5, absolute_threshold = 5000)
View(IPEX_results_ignorm)
View(SASH3_results_ignorm)
View(SASH3_results_qnorm)
############### Results
# SASH3 qnorm
SASH3_results_qnorm <- detect.hits(q_data, controls = "BD", group_vector = groupsv,
examine = "SASH3", sdmean = 0.2, fold_threshold = 5, absolute_threshold = 5000)
# SASH3 ignorm
SASH3_results_ignorm <- detect.hits(ig_data, controls = "BD", group_vector = groupsv,
examine = "SASH3", sdmean = 0.2, fold_threshold = 5, absolute_threshold = 5000)
############### Results
# SASH3 qnorm
SASH3_results_qnorm <- detect.hits(q_data, controls = "BD", group_vector = groupsv,
examine = "SASH3", sdmean = 1, fold_threshold = 5, absolute_threshold = 5000)
############### Results
# SASH3 qnorm
SASH3_results_qnorm <- detect.hits(q_data, controls = "BD", group_vector = groupsv,
examine = "SASH3", sdmean = 0.5, fold_threshold = 5, absolute_threshold = 5000)
# SASH3 ignorm
SASH3_results_ignorm <- detect.hits(ig_data, controls = "BD", group_vector = groupsv,
examine = "SASH3", sdmean = 0.5, fold_threshold = 5, absolute_threshold = 5000)
View(SASH3_results_ignorm)
View(SASH3_results_qnorm)
View(SASH3_results_ignorm)
############### Results
# SASH3 qnorm
SASH3_results_qnorm <- detect.hits(q_data, controls = "BD", group_vector = groupsv,
examine = "SASH3", sdmean = 0.5, fold_threshold = 5, absolute_threshold = 5000, min_mean = 50)
############### Results
# SASH3 qnorm
SASH3_results_qnorm <- detect.hits(q_data, controls = "BD", group_vector = groupsv,
examine = "SASH3", sdmean = 0.5, fold_threshold = 5, absolute_threshold = 5000)
############### Results
# SASH3 qnorm
SASH3_results_qnorm <- detect.hits(q_data, controls = "BD", group_vector = groupsv,
examine = "SASH3", sdmean = 0.5, fold_threshold = 5, absolute_threshold = 5000, min_mean = 50)
# SASH3 ignorm
SASH3_results_ignorm <- detect.hits(ig_data, controls = "BD", group_vector = groupsv,
examine = "SASH3", sdmean = 0.5, fold_threshold = 5, absolute_threshold = 5000, min_mean = 50)
# RELB qnorm
RELB_results_qnorm <- detect.hits(q_data, controls = "BD", group_vector = groupsv,
examine = "RELB", sdmean = 0.5, fold_threshold = 5, absolute_threshold = 5000, min_mean = 50)
# RELB ignorm
RELB_results_ignorm <- detect.hits(ig_data, controls = "BD", group_vector = groupsv,
examine = "RELB", sdmean = 0.5, fold_threshold = 5, absolute_threshold = 5000, min_mean = 50)
#IPEX qnorm
IPEX_results_qnorm <- detect.hits(q_data, controls = "BD", group_vector = groupsv,
examine = "IPEX", sdmean = 0.5, fold_threshold = 5, absolute_threshold = 5000, min_mean = 50)
#IPEX ignorm
IPEX_results_ignorm <- detect.hits(ig_data, controls = "BD", group_vector = groupsv,
examine = "IPEX", sdmean = 0.5, fold_threshold = 5, absolute_threshold = 5000, min_mean = 50)
